{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elanco Lyo Trials\n",
    "Read in datafiles & plot relevant variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add libraries, etc.\n",
    "%run -i commonRoutines.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run -i pulseFunctions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# each folder in directory contains files to merge\n",
    "dirname = r\"\\\\ion\\ion.grp\\ElancoGlobalEng\\AugustaLyoTrials\\1 Study Data Files (SYMYX)\"\n",
    "folders = os.listdir(dirname) # list of folders w datafiles\n",
    "folders.sort() # sort to load data in StudyRun order\n",
    "dataPath = r\"Data\\Exported\" # datafiles to be stitched are in this folder\n",
    "mergePath = r\"Data\" # stitched data will be written to this folder\n",
    "\n",
    "# dataframes to read files into\n",
    "sysFrame = pd.DataFrame()\n",
    "probFrame = pd.DataFrame()\n",
    "sysList_ = []\n",
    "probList_ = []\n",
    "dataIndex = 0\n",
    "\n",
    "# iterate through list of folders & load data\n",
    "for folder_ in folders:\n",
    "    \n",
    "    # don't load data from template folder\n",
    "    if 'CopyPasteTemplate' in folder_:\n",
    "        continue\n",
    "    if '.db' in folder_:\n",
    "        continue\n",
    "    if 'New folder' in folder_:\n",
    "        continue\n",
    "    if 'PS' in folder_:\n",
    "        continue\n",
    "\n",
    "        \n",
    "    # increment dataIndex for each folder\n",
    "    dataIndex += 1\n",
    "    print('Loading %d: %s' % (dataIndex, folder_))\n",
    "\n",
    "    #check if dataset already loaded\n",
    "    checkPath = os.path.join(dirname, folder_, mergePath)\n",
    "    checkFiles = glob.glob(checkPath + \"/*.csv\")\n",
    "    merged = False\n",
    "    for file_ in checkFiles:\n",
    "        if 'Export' in file_:\n",
    "            merged = True\n",
    "            print('%s already merged' % folder_)\n",
    "    if merged == True:\n",
    "        path = os.path.join(dirname, folder_, mergePath)\n",
    "        headerRows = 0\n",
    "    else:    \n",
    "        # create list of files to merge\n",
    "        path = os.path.join(dirname, folder_, dataPath)\n",
    "        headerRows = 9\n",
    "  \n",
    "    filenames = glob.glob(path + \"/*.csv\")\n",
    "    \n",
    "    # iterate through files & read in data, appending to appropriate dataframe\n",
    "    for file_ in filenames:\n",
    "        \n",
    "        df = pd.read_csv(file_, index_col=None, header=headerRows)\n",
    "        \n",
    "        #add datetime column\n",
    "        df['datetime'] = df['Date    '] + \" \" + df[' Time    ']\n",
    "        try:\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'], format='%m/%d/%y %H:%M:%S')\n",
    "        except:\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'], format='%m/%d/%Y %H:%M:%S')\n",
    "        \n",
    "        # add column indexing each dataset read in\n",
    "        df['dataset'] = dataIndex\n",
    "        df['folderName'] = folder_\n",
    "        \n",
    "        #append dataset to list        \n",
    "        if 'SYS' in file_:\n",
    "            #print('Adding ' + file_ + ' to SYS')\n",
    "            sysList_.append(df)\n",
    "        elif 'PRO' in file_:\n",
    "            #print('Adding ' + file_ + ' to PROB')\n",
    "            probList_.append(df)\n",
    "\n",
    "sysFrame = pd.concat(sysList_)\n",
    "probFrame = pd.concat(probList_)\n",
    "    \n",
    "print('Data loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove duplicate rows\n",
    "print('...deleting duplicates...')\n",
    "sysFrame = sysFrame.drop_duplicates(['datetime'])\n",
    "probFrame = probFrame.drop_duplicates(['datetime'])\n",
    "sysFrame = sysFrame[sysFrame[' ENCORE:VAC4.F_CV (S)'] != '        ???          ']\n",
    "probFrame = probFrame[probFrame[' ENCORE:TPAVG.F_CV (S)'] != '        ???          ']\n",
    "    \n",
    "# convert columns to correct datatype\n",
    "print('...setting datatypes...')\n",
    "sysFrame[[' ENCORE:TEMPCOND1.F_CV (S)', ' ENCORE:TEMPSHELF.F_CV (S)', ' ENCORE:TEMPSREF.F_CV (S)', ' ENCORE:TPAVG.F_CV (S)', \n",
    "         ' ENCORE:VAC.F_CV (S)', ' ENCORE:VAC2.F_CV (S)', \n",
    "         ' ENCORE:VAC4.F_CV (S)']] = sysFrame[[' ENCORE:TEMPCOND1.F_CV (S)', \n",
    "                                                           ' ENCORE:TEMPSHELF.F_CV (S)', ' ENCORE:TEMPSREF.F_CV (S)', ' ENCORE:TPAVG.F_CV (S)', \n",
    "                                                           ' ENCORE:VAC.F_CV (S)', ' ENCORE:VAC2.F_CV (S)', \n",
    "                                                           ' ENCORE:VAC4.F_CV (S)']].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "#add diff between MKS & pirani\n",
    "sysFrame['diff'] = sysFrame[' ENCORE:VAC2.F_CV (S)'] - sysFrame[' ENCORE:VAC.F_CV (S)']\n",
    "\n",
    "print('...sorting data...')\n",
    "sysFrame = sysFrame.sort_values(['datetime'], ascending=True)\n",
    "probFrame = probFrame.sort_values(['datetime'], ascending=True)\n",
    "\n",
    "# merge & reindex dataframes\n",
    "print('...reindexing...')\n",
    "sysFrame['index'] = np.arange(len(sysFrame))\n",
    "sysFrame.set_index('index', inplace=True)\n",
    "probFrame['index'] = np.arange(len(probFrame))\n",
    "probFrame.set_index('index', inplace=True)\n",
    "\n",
    "# check time from beginning\n",
    "sysFrame['dTime'] = sysFrame['datetime'] - sysFrame['datetime'][0]\n",
    "\n",
    "# view available variables\n",
    "sysFrame.columns\n",
    "\n",
    "#sysFrame[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write merged data sets to folder\n",
    "writeDataIndex = 0\n",
    "\n",
    "# group data\n",
    "dataGroup = sysFrame.groupby(['dataset'])\n",
    "probDataGroup = probFrame.groupby(['dataset'])\n",
    "\n",
    "for folder_ in folders:\n",
    "    \n",
    "    # skip over template folder\n",
    "    if 'CopyPasteTemplate' in folder_:\n",
    "        continue\n",
    "    if 'db' in folder_:\n",
    "        continue\n",
    "    if 'New folder' in folder_:\n",
    "        continue\n",
    "    if 'PS' in folder_:\n",
    "        continue\n",
    "        \n",
    "    writeDataIndex += 1\n",
    "    path = os.path.join(dirname, folder_)\n",
    "    \n",
    "    #check if merged file already exists\n",
    "    checkPath = os.path.join(dirname, folder_, mergePath)\n",
    "    checkFiles = glob.glob(checkPath + \"/*.csv\")\n",
    "    merged = False\n",
    "    for file_ in checkFiles:\n",
    "        if 'Export' in file_:\n",
    "            merged = True\n",
    "    if merged == True:\n",
    "        print('%s merge files already written' % folder_)\n",
    "        continue\n",
    "        \n",
    "    # merge file not already created. write to csv\n",
    "    sys = dataGroup.get_group(writeDataIndex)\n",
    "    prob = probDataGroup.get_group(writeDataIndex)\n",
    "    lastDate = pd.to_datetime(sys['datetime'][-1:].values[0])\n",
    "    dateString = '%02d%02d%d' % ( lastDate.day, lastDate.month, lastDate.year - 2000)\n",
    "    sysFilename = os.path.join(checkPath, '%s%sSYSExport.csv' % (dateString, folder_))\n",
    "    probFilename = os.path.join(checkPath, '%s%sPROBExport.csv' % (dateString, folder_))\n",
    "    \n",
    "    print('Writing %s' % folder_)\n",
    "    sys[['Date    ', ' Time    ', ' ENCORE:TEMPCOND1.F_CV (S)',\n",
    "       ' ENCORE:TEMPSHELF.F_CV (S)', ' ENCORE:TEMPSREF.F_CV (S)',\n",
    "       ' ENCORE:TPAVG.F_CV (S)', ' ENCORE:VAC.F_CV (S)',\n",
    "       ' ENCORE:VAC2.F_CV (S)', ' ENCORE:VAC4.F_CV (S)',\n",
    "       ' ENCORE:VACREF.F_CV (S)']].to_csv(sysFilename, index=False)\n",
    "    prob[['Date    ', ' Time    ', ' ENCORE:TP01.F_CV (S)',\n",
    "       ' ENCORE:TP02.F_CV (S)', ' ENCORE:TP03.F_CV (S)',\n",
    "       ' ENCORE:TP04.F_CV (S)', ' ENCORE:TP05.F_CV (S)',\n",
    "       ' ENCORE:TP06.F_CV (S)', ' ENCORE:TP07.F_CV (S)',\n",
    "       ' ENCORE:TP08.F_CV (S)', ' ENCORE:TPAVG.F_CV (S)']].to_csv(probFilename, index=False)\n",
    "    \n",
    "print('*** Finished writing files ***')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rename columns\n",
    "sysFrame = sysFrame.rename(columns={'Date    ' : 'Date', ' Time    ' : 'Time', ' ENCORE:TEMPCOND1.F_CV (S)' : 'CondenserT',\n",
    "       ' ENCORE:TEMPSHELF.F_CV (S)' : 'ShelfT', ' ENCORE:TEMPSREF.F_CV (S)' : 'RefT',\n",
    "       ' ENCORE:TPAVG.F_CV (S)' : 'AvgT', ' ENCORE:VAC.F_CV (S)' : 'MKS',\n",
    "       ' ENCORE:VAC2.F_CV (S)' : 'Pirani', ' ENCORE:VAC4.F_CV (S)' : 'Condenser',\n",
    "       ' ENCORE:VACREF.F_CV (S)' : 'VacRef'})\n",
    "\n",
    "# grouped variable used to subset data by dataset\n",
    "print('...grouping datasets...')\n",
    "#dataGroup = data.groupby(['dataset'])\n",
    "dataGroup = sysFrame.groupby(['dataset'])\n",
    "probDataGroup = probFrame.groupby(['dataset'])\n",
    "print('Grouped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Augusta run for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Augusta = pd.read_csv(r\"C:\\Users\\c213418\\Documents\\Elanco\\Elanco\\AUGUSTA_TEST_01.csv\", index_col=None, header=0)\n",
    "\n",
    "Augusta['datetime'] = pd.to_datetime(Augusta['Unnamed: 0'], infer_datetime_format=True)\n",
    "\n",
    "Augusta = Augusta.rename(columns = {'Pulse Number' : 'PulseNumber', 'Chamber Vacuum' : 'ChamberVac', 'Condesner Coil Temp' : 'CondenserT',\n",
    "       'Shelf Inlet Temp' : 'ShelfTIn', 'Shelf Outlet Temp' : 'ShelfTOut'})\n",
    "\n",
    "Augusta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Find Cycle Start Time & index\n",
    "\n",
    "# Augusta data\n",
    "searchStart = 0 # indicates status of search\n",
    "cycleStart_Augusta = 0 # index of start of cycle\n",
    "\n",
    "for i in Augusta.index:\n",
    "    if searchStart == 0: # looking for initial drop in T\n",
    "        if Augusta['ShelfTIn'][i] < -43:\n",
    "            searchStart = 1\n",
    "    if searchStart == 1: \n",
    "        if Augusta['ShelfTIn'][i] < -50: # confirm not going to holding T\n",
    "            searchStart = 0\n",
    "        if Augusta['ShelfTIn'][i] > -38: #-41.5: # start indicated by return up\n",
    "            cycleStart_Augusta = i\n",
    "            searchStart = 0\n",
    "            break\n",
    "            \n",
    "Augusta['dTime'] = Augusta['datetime'] - Augusta['datetime'][cycleStart_Augusta]\n",
    "\n",
    "##  StudyXRunY data\n",
    "# initialize dataframe to hold results\n",
    "cycleStart_runx = pd.DataFrame(index=np.arange(0,dataIndex), columns=('Study Number', 'cycleOffset', 'cycleStart', 'cycleStartTime'))\n",
    "cycleStartIndex = 0\n",
    "\n",
    "## compare cycle start to Justin/Karl's calculations\n",
    "startTimes = ['2016-12-05 20:54:00', '2016-12-07 20:02:00', '2016-12-09 19:34:50', '2016-12-12 20:13:40', '2016-12-14 20:11:50',\n",
    "              '2016-12-16 19:50:20', '2016-10-19 20:51:00', '2016-10-24 21:10:00', '2016-10-26 22:05:00', '2016-10-31 18:35:00',\n",
    "              '2016-11-02 19:18:00', '2016-11-07 19:36:17', '2016-11-09 20:25:00', '2016-11-14 19:55:00', '2016-11-16 21:05:00', \n",
    "              '2016-11-18 20:24:00', '2016-11-21 20:23:00', '2016-12-02 19:01:50']\n",
    "\n",
    "startTimes = pd.to_datetime(startTimes)\n",
    "\n",
    "# loop through each dataset to determine cycle start\n",
    "for index, group in dataGroup:\n",
    "    dropT = -43.9\n",
    "    print('Searching %d for cycle start' %index)\n",
    "    first= False\n",
    "    if (group['folderName'][-1:].item()==\"Study20Run1\"):\n",
    "        dropT = -29.9\n",
    "        first = True \n",
    "    for i in group.index:\n",
    "        if searchStart == 0: # looking for drop to -44\n",
    "            if group['RefT'][i] < dropT:\n",
    "                searchStart = 1\n",
    "        if searchStart == 1: # checking that temp dropped to -44 & held before ramping up\n",
    "            if group['RefT'][i] < -50:\n",
    "                searchStart = 2 # not start of cycle, look again\n",
    "            elif (group['folderName'][-1:].item()==\"Study20Run1\") & first:\n",
    "                first = False\n",
    "                searchStart = 2\n",
    "            elif group['RefT'][i] > dropT: #-41.5: # start of cycle found (38)\n",
    "                cycleStartIndex = i\n",
    "                cycleStart_runx['Study Number'][index-1] = group['folderName'][-1:].item()\n",
    "                cycleStart_runx['cycleOffset'][index-1] = i - group.index[0]\n",
    "                cycleStart_runx['cycleStart'][index-1] = i\n",
    "                cycleStart_runx['cycleStartTime'][index-1] = group['datetime'][i]\n",
    "                searchStart = 0\n",
    "                break\n",
    "        if searchStart == 2: # drop to -44 not start of cycle, reset to look again\n",
    "            if group['RefT'][i] > -20:\n",
    "                searchStart = 0\n",
    "run_shift = cycleStart_Augusta - cycleStart_runx['cycleOffset']\n",
    "\n",
    "cycleStart_runx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "piraniData = pd.DataFrame(index=np.arange(0,dataIndex), columns=('Study Number', 'Pirani Start', 'Pirani End', 'MKS_Start', \n",
    "                                                                'diffStart', 'diffEnd', 'diff50', 'Offset', 'Target', 'Target2'))\n",
    "\n",
    "for index, group in dataGroup:\n",
    "    # collect variables for each dataset (studyName, pirani/diff data)\n",
    "    print(group['folderName'][-1:].item())\n",
    "    piraniData['Study Number'][index-1] = group['folderName'][-1:].item()\n",
    "    #pirani_moving = movingAverage(group1['Pirani'], 20)\n",
    "    group['piraniFiltered'] = firstOrder(group['Pirani'], 80, 0.2)\n",
    "    group['MKSFiltered'] = firstOrder(group['MKS'], 80, 0.2)\n",
    "    group['diffFiltered'] = firstOrder(group['diff'], 80, 0.2)\n",
    "    shift = group.index[0] # to access correct index in looping (indexing does not start at 0 for each new set)\n",
    "    cycleStart = cycleStart_runx['cycleStart'][index-1] - shift\n",
    "    \n",
    "    \n",
    "    # scan for measurements... \n",
    "    pulseCheck = np.amin(group['Pirani'][cycleStart+2000:cycleStart+5000])\n",
    "    if pulseCheck < 400: # cycle has pulses\n",
    "        target = determineTargetDiff(cycleStart)\n",
    "    else:\n",
    "        target = determineTargetDiffNoPulse(cycleStart)\n",
    "    \n",
    "piraniData[['Study Number', 'diffStart', 'MKS_Start', 'Offset', 'Target2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cycle data\n",
    "\n",
    "# initialize dataframes to hold results & indices of results\n",
    "\n",
    "cycleData = pd.DataFrame(index=np.arange(0,dataIndex), columns=('Study Number', 'Cycle Start Time', 'Primary Duration', \n",
    "                                                                'pirani', 'Pir dry end time', 'Pulse Count', \n",
    "                                                                'Product Approach Shelf T', 'length', 'lengthTotal',\n",
    "                                                                'Cycle Length', 'Cycle End Time', 'AugustaAvgTime',\n",
    "                                                               'Augusta Average Time', 'TimeSaved', 'Time Saved'))\n",
    "cycleIndices = pd.DataFrame(index=np.arange(0,dataIndex), columns=('Cycle Start', 'Pirani Transition', \n",
    "                                                                'Product Approach Shelf T', 'Cycle End'))\n",
    "\n",
    "AugustaTime = pd.Timedelta('32 hours 1 min')\n",
    "cycleData['AugustaAvgTime'] = AugustaTime\n",
    "cycleData['Augusta Average Time'] = '%d hours %d minutes' % (AugustaTime.total_seconds()//3600,\n",
    "                                                        (AugustaTime.total_seconds()%3600)//60)\n",
    "\n",
    "# find transition from primary to secondary drying\n",
    "for index, group in dataGroup:\n",
    "    \n",
    "    cycleData['Study Number'][index-1] = group['folderName'][-1:].item()\n",
    "    cycleData['Cycle Start Time'][index-1] = cycleStart_runx['cycleStartTime'][index-1]\n",
    "    cycleIndices['Cycle Start'][index-1] = cycleStart_runx['cycleStart'][index-1]\n",
    "    shift = group.index[0]\n",
    "    cycleStart = cycleStart_runx['cycleStart'][index-1] - shift\n",
    "    MKStart = piraniData['MKS_Start'][index-1]\n",
    "    diff50 = piraniData['Target2'][index-1]\n",
    "    diffEnd = piraniData['diffEnd'][index-1]\n",
    "    transitionPulse = pd.DataFrame()\n",
    "    \n",
    "    if group['folderName'][-1:].item()==\"Study8Run1\" : #vacuum did not pull all the way down..\n",
    "        diffEnd += 20\n",
    "\n",
    "    # search for Pirani transition\n",
    "    pulseCheck = np.amin(group['Pirani'][cycleStart+2000:cycleStart+5000])\n",
    "    print(pulseCheck)\n",
    "    if pulseCheck < 400:\n",
    "        transition = pulseTransition(group, index, shift, cycleStart, MKStart, diff50, diffEnd)\n",
    "    else:\n",
    "        transition = noPulseTransition(group, index, shift, cycleStart, MKStart, diff50, diffEnd)\n",
    "    \n",
    "    \n",
    "    # find end of cycle\n",
    "    print('Searching %d for end of cycle' %index)\n",
    "    finalSearchIndex = transition - shift\n",
    "    for i in group.index[finalSearchIndex:]:\n",
    "        if group['ShelfT'][i] < 20:\n",
    "            totalTime = group['datetime'][i] - cycleData['Cycle Start Time'][index-1]\n",
    "            cycleData['lengthTotal'][index-1] = totalTime\n",
    "            cycleData['Cycle Length'][index-1] = '%d hours %d minutes' % (totalTime.total_seconds()//3600,\n",
    "                                                                        (totalTime.total_seconds()%3600)//60)\n",
    "            cycleIndices['Cycle End'][index-1] = i\n",
    "            cycleData['Cycle End Time'][index-1] = group['datetime'][i]\n",
    "            \n",
    "            cycleData['TimeSaved'][index-1] = cycleData['AugustaAvgTime'][index-1] - cycleData['lengthTotal'][index-1]\n",
    "            cycleData['Time Saved'][index-1] = '%d hours %d minutes' % (cycleData['TimeSaved'][index-1].total_seconds()//3600,\n",
    "                                                                (cycleData['TimeSaved'][index-1].total_seconds()%3600)//60)\n",
    "            break\n",
    "\n",
    "\n",
    "cycleData[['Study Number', 'Cycle Start Time', 'Primary Duration', 'Pir dry end time', 'Cycle Length', 'Cycle End Time', 'Augusta Average Time', 'Time Saved']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cycleData[['Study Number', 'Cycle Start Time', 'Primary Duration', 'Pir dry end time', 'Cycle Length', 'Cycle End Time', 'Augusta Average Time', 'Time Saved']].to_csv('primaryDuration2_08FEB2017.csv')\n",
    "#piraniData[['Study Number', 'diffStart', 'MKS_Start', 'Offset', 'Target2']].to_csv('piraniTransition_23JAN.csv')\n",
    "#a = [False, True, False]\n",
    "\n",
    "#startTimes = cycleData['Cycle Start Time']\n",
    "#startTimes = ['2016-12-05 20:54:00', '2016-12-07 20:02:00', '2016-12-09 19:34:50', '2016-12-12 20:13:40', '2016-12-14 20:11:50', '2016-12-16 19:50:20', \n",
    "#             '2016-10-19 20:51:00', '2016-10-24 21:10:00', '2016-10-26 22:05:00', '2016-10-31 18:35:00' , '2016-11-02 19:18:00', '2016-11-07 19:36:17', \n",
    "#             '2016-11-09 20:25:00', '2016-11-14 19:55:00', '2016-11-16 21:05:00', '2016-11-18 20:24:00', '2016-11-21 20:23:00', '2016-12-02 19:01:50', NaN]\n",
    "\n",
    "#startTimes = pd.to_datetime(startTimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp = PdfPages(r'\\\\ion\\ion.grp\\ElancoGlobalEng\\AugustaLyoTrials\\2 Results and Report File (SYMXYX)\\Analysis\\primaryDuration_22FEB2017.pdf')\n",
    "\n",
    "for index, group in dataGroup:\n",
    "    studyName = group['folderName'][-1:].item()\n",
    "    cycleStartIndex = cycleStart_runx['cycleStart'][index-1]\n",
    "    group['dTime'] = group['datetime'] - group['datetime'][cycleStartIndex]\n",
    "    startCalc = cycleData['Cycle Start Time'][index-1] - group['datetime'][cycleStartIndex]\n",
    "    group['dTime'] = group['dTime'] / timedelta(days=1) * 24 # convert timedelta object to hours\n",
    "    diff50 = piraniData['Target2'][index-1]\n",
    "    \n",
    "    if studyName==\"Study9Run1\" or studyName==\"Study9Run2\":\n",
    "        continue\n",
    "    #fig = plt.figure(index)\n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    sub = dataIndex * 100 + 10 + index\n",
    "    sub = 111\n",
    "    ax1 = fig.add_subplot(sub)\n",
    "    \n",
    "    ax1.set_title('%s' %studyName)\n",
    "    ax1.set_ylabel('Temperature (deg C)')\n",
    "    ax1.set_ylim(-90, 70)\n",
    "    #ax1.set_ylim(-50, -30)\n",
    "    ax1.set_xlabel('Time (hrs)')\n",
    "    \n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Pressure (mTorr)')\n",
    "    ax2.set_ylim(0, 3000)\n",
    "    \n",
    "    start = cycleIndices['Cycle Start'][index-1] #- group.index[0]\n",
    "    trans = cycleIndices['Pirani Transition'][index-1]\n",
    "    end = cycleIndices['Cycle End']\n",
    "    \n",
    "    ln1 = ax1.plot(Augusta['dTime'] / timedelta(days=1) * 24, Augusta['ShelfTIn'], color=\"orange\", label=\"Augusta\")\n",
    "    ln2 = ax1.plot(group['dTime'], group['ShelfT'], color=\"green\", label='%s Shelf T' %studyName)\n",
    "    ax1.plot(group['dTime'][start], group['ShelfT'][start], 'ro')\n",
    "    #ax1.axvline(startCalc / datetime.timedelta(days=1) * 24)\n",
    "    ln4 = ax2.plot(group['dTime'], group['Pirani'], color=\"red\", label=\"Pirani\")\n",
    "    ln3 = ax1.plot(group['dTime'], group['AvgT'], color=\"blue\", label=\"Product Temp\")\n",
    "    ln5 = ax2.plot(group['dTime'], group['MKS'], color = \"blue\", label = \"MKS\")\n",
    "    ax2.plot(group['dTime'], group['diff'], color=\"green\", label = \"diff\")\n",
    "    if not (math.isnan(trans)):\n",
    "        #ax2.plot(group['dTime'][trans], group['Pirani'][trans], 'ro')\n",
    "        ax2.plot(group['dTime'][trans], diff50, 'ro')\n",
    "    \n",
    "    ax2.axhline(piraniData['Target2'][index-1], color=\"red\")\n",
    "    ax2.axhline(piraniData['diffStart'][index-1])\n",
    "    ax2.axhline(piraniData['Offset'][index-1])\n",
    "    \n",
    "    ax1.set_xlim(-10, 40)\n",
    "    ax2.set_xlim(-10, 40)\n",
    "    #ax1.set_xlim(-1, 1)\n",
    "    #ax2.set_xlim(-1, 1)\n",
    "    lns = ln1+ln2+ln3+ln4+ln5\n",
    "    #lns = ln2\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    #ax2.legend(lns, labs) #, bbox_to_anchor=(1.05, 1), loc=2)\n",
    "    # Shrink current axis by 20%\n",
    "    box = ax1.get_position()\n",
    "    ax1.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "    ax2.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "    ax1.legend(lns, labs, bbox_to_anchor=(1.05, 1), loc=2)\n",
    "    \n",
    "    #plt.show() \n",
    "    plt.savefig(pp, format='pdf')\n",
    "    \n",
    "#plt.show()\n",
    "#plt.savefig(pp, format='pdf')\n",
    "pp.close()\n",
    "print('Saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
